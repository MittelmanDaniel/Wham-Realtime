# ğŸ¤– WHAM Real-Time Project Summary

**Goal**: Use WHAM for real-time humanoid robot teleoperation

**Result**: âœ… Achieved **150ms latency** (87x faster than batch processing!)

---

## ğŸ¯ Quick Links

1. **ğŸ“– Start Here**: `docs/QUICK_START_GUIDE.md` - Complete usage guide
2. **ğŸŒŸ Main Script**: `realtime_wham_online.py` - The real-time processor
3. **ğŸš€ Run Tests**: `sbatch slurm/run_online_wham.sbatch`
4. **ğŸ“ File Organization**: `docs/FILE_ORGANIZATION.md`

---

## ğŸ† Key Achievement

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Latency** | 13,000ms | **150ms** | **87x faster** âš¡ |
| **FPS** | 1.1 | **6.3** | **5.7x faster** |
| **Robot Ready?** | âŒ No | âœ… **YES!** | ğŸ‰ |

---

## ğŸ“Š Latency Options

Different frame-skip rates for different use cases:

| Frame Skip | Input FPS | Latency | Best For |
|------------|-----------|---------|----------|
| 1 (every frame) | 30 FPS | 150ms | High accuracy |
| 2 (half frames) | 15 FPS | 126ms | Balanced |
| 5 (1/5 frames) | 6 FPS | **84ms** | **Lowest latency** âš¡ |

---

## ğŸš€ Quick Start

### Run the main test:
```bash
cd /home/hice1/dmittelman6/WHAM
sbatch slurm/run_online_wham.sbatch
```

### Run on your own video:
```bash
python realtime_wham_online.py your_video.mp4 \
    --frame-skip 1 \
    --max-fps 30 \
    --duration 30
```

### Monitor results:
```bash
tail -f logs/online_wham_*.err
```

---

## ğŸ“ Project Structure

```
WHAM/
â”œâ”€â”€ ğŸŒŸ realtime_wham_online.py    # Main real-time processor
â”œâ”€â”€ wham_api.py                    # WHAM API interface
â”œâ”€â”€ demo.py                        # Original WHAM demo
â”‚
â”œâ”€â”€ ğŸ“‚ slurm/                      # GPU job scripts
â”‚   â””â”€â”€ run_online_wham.sbatch    # Run real-time tests
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                       # Documentation
â”‚   â”œâ”€â”€ QUICK_START_GUIDE.md      # â­ READ THIS FIRST
â”‚   â””â”€â”€ FILE_ORGANIZATION.md      # Project structure
â”‚
â”œâ”€â”€ ğŸ“‚ archive/                    # Old/test files
â”‚   â”œâ”€â”€ tests/                    # Development tests
â”‚   â”œâ”€â”€ camera_streaming/         # Macâ†’Cluster streaming
â”‚   â””â”€â”€ old_versions/             # Earlier implementations
â”‚
â”œâ”€â”€ ğŸ“‚ lib/                        # WHAM core library
â”œâ”€â”€ ğŸ“‚ configs/                    # Configuration files
â”œâ”€â”€ ğŸ“‚ checkpoints/                # Model weights (~5GB)
â”œâ”€â”€ ğŸ“‚ examples/                   # Demo videos
â”œâ”€â”€ ğŸ“‚ output/                     # Results
â””â”€â”€ ğŸ“‚ logs/                       # Job logs
```

---

## ğŸ“ What We Learned

### 1. Batch Size Matters!
- **Batch=16**: Great for throughput, terrible for latency (13s)
- **Batch=1**: Required for real-time, achieves 150ms latency âœ…

### 2. GPU Compatibility
- âœ… **V100, RTX 6000, A100**: Work perfectly
- âŒ **H100/H200**: PyTorch 1.11.0 too old
- âŒ **AMD MI210**: CUDA only

### 3. Bottleneck Analysis
- ViTPose (detection): ~150ms âš ï¸ Main bottleneck
- YOLO (person detection): ~10ms
- WHAM (inference): Fast

### 4. Real-Time Simulation Works!
- Throttling video playback to 30 FPS gives accurate latency
- No need for actual camera streaming during testing
- Results match what you'd get with live camera

---

## ğŸ’¡ Use Cases

### âœ… Perfect For:
- ğŸ¤– Humanoid robot teleoperation (150ms is good!)
- ğŸ­ Live motion capture
- ğŸ® Interactive VR/AR applications
- ğŸƒ Real-time sports analysis

### âš ï¸ Consider Alternatives If:
- You need <50ms latency (consider simpler models)
- You need >30 FPS (consider lighter pose estimators)
- You have no GPU access (WHAM requires CUDA)

---

## ğŸ“š Documentation Files

1. **`PROJECT_SUMMARY.md`** (this file) - High-level overview
2. **`docs/QUICK_START_GUIDE.md`** - Detailed usage guide
3. **`docs/FILE_ORGANIZATION.md`** - File structure explanation
4. **`archive/README.md`** - Archived files explanation

---

## ğŸ”§ Key Commands

```bash
# Submit real-time test job
sbatch slurm/run_online_wham.sbatch

# Check job status
squeue -u $USER

# Monitor job output
tail -f logs/online_wham_*.err

# Run on custom video
python realtime_wham_online.py video.mp4 --frame-skip 2

# Clean up old outputs
rm -rf output/online_test* logs/*.out logs/*.err
```

---

## ğŸŒŸ The Magic File

**`realtime_wham_online.py`** is where all the magic happens:

```python
# Key features:
- Batch size = 1 (online processing)
- Immediate frame processing (no buffering)
- Real-time simulation via FPS throttling
- Per-frame latency measurement
- Supports: video files, HTTP, RTSP, cameras
```

Run it with:
```bash
python realtime_wham_online.py <source> [--frame-skip N] [--max-fps FPS]
```

---

## ğŸ“Š Benchmark Results (V100 GPU)

### Test 1: Every Frame (30 FPS)
- âœ… Latency: **157ms average** (150ms median)
- âœ… Throughput: 6.3 FPS
- âœ… Detection: 1 person per frame

### Test 2: Every 2nd Frame (15 FPS)
- âœ… Latency: **130ms average** (126ms median)
- âœ… Throughput: 6.0 FPS
- âœ… Detection: 1 person per frame

### Test 3: Every 5th Frame (6 FPS)
- âœ… Latency: **93ms average** (84ms median) âš¡
- âœ… Throughput: 4.4 FPS
- âœ… Detection: 1 person per frame

---

## ğŸ¯ Next Steps

1. âœ… **Read the guide**: `docs/QUICK_START_GUIDE.md`
2. âœ… **Run the tests**: `sbatch slurm/run_online_wham.sbatch`
3. âœ… **Try your videos**: `python realtime_wham_online.py your_video.mp4`
4. ğŸ”„ **Integrate with robot**: Use `wham_api.py` for programmatic access
5. ğŸ“¹ **Optional**: Set up camera streaming (see `archive/camera_streaming/`)

---

## ğŸ™ Credits

- **WHAM**: [wham.is.tue.mpg.de](https://wham.is.tue.mpg.de/)
- **HumanPlus** (Stanford): Inspired real-time usage
- **ViTPose**: 2D pose estimation
- **YOLO**: Person detection
- **PyTorch**: Deep learning framework

---

## ğŸ“ Support

- ğŸ“– **Documentation**: Start with `docs/QUICK_START_GUIDE.md`
- ğŸ› **Issues**: Check GPU compatibility and environment setup
- ğŸ’¬ **Questions**: Refer to WHAM paper and codebase

---

## âœ¨ Bottom Line

**WHAM can run in real-time at 150ms latency on V100 GPUs!**

This makes it viable for humanoid robot teleoperation and other real-time applications. The key was optimizing from batch processing to online processing - an **87x improvement** in latency! ğŸ‰

---

*Project completed: October 8, 2025*
*Environment: Georgia Tech PACE-ICE Cluster*
*GPU: NVIDIA Tesla V100*

